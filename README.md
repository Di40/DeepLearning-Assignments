# Deep Learning Assignments

1. [From the Perceptron to DNN](https://github.com/Di40/DeepLearning-Assignments/blob/main/HW1%20-%20From%20the%20Perceptron%20to%20DNN.ipynb)

From understanding a simple perceptron to building a basic **feedforward neural network** for digit classification using Python and NumPy.

2. [Optimize and Train Deep Models](https://github.com/Di40/DeepLearning-Assignments/blob/main/HW2%20-%20Optimize%20and%20Train%20Deep%20Models.ipynb)

Development of a basic **Deep Neural Network** for a classification problem using [PyTorch ðŸ”¥](https://pytorch.org/). Handle **overfitting** on the training set and conduct fair model selection.

3. [Convolutional Neural Networks](https://github.com/Di40/DeepLearning-Assignments/blob/main/HW3%20-%20Convolutional%20Neural%20Networks.ipynb)

Develop a **Convolutional Neural Network** for image classification on the CIFAR-10 dataset.

![image](https://github.com/Di40/DeepLearning-Assignments/assets/57565142/11dfb391-041e-44f4-a71c-6a72f759e2f3)

4. [Recurrent Neural Networks & Transformer](https://github.com/Di40/DeepLearning-Assignments/blob/main/HW4%20-%20Recurrent%20Neural%20Networks%20%26%20Transformer.ipynb)

Develop a **Recurrent Neural Network (RNN)** for **sentiment analysis** using the IMDB dataset. Compare a simple RNN with more advanced recurrent models such as **LSTM** and **GRU** in terms of computational load, the number of parameters, and performance. Additionally, experiment with a bidirectional model to uncover the strengths and weaknesses of this technique. Finally, solve the same classification problem with a **Transformer** to gain a deeper understanding of its internal workings.

5. [Autoencoders](https://github.com/Di40/DeepLearning-Assignments/blob/main/HW5%20-%20Autoencoders.ipynb)

Develop a simple shallow autoencoder for **dimensionality reduction**. Next, create a deep version. Finally, experiment with applying autoencoders to denoising data tasks (**denoising autoencoder**).

![image](https://github.com/Di40/DeepLearning-Assignments/assets/57565142/8efaa257-edcb-419e-a853-e32e5199d0ea)

6. [Variational Autoencoders](https://github.com/Di40/DeepLearning-Assignments/blob/main/HW6%20-%20Variational%20Autoencoders.ipynb)

Define a **Variational Autoencoder (VAE)** using fundamental PyTorch components. Next, establish a training loop that includes the two essential losses for VAE training: the **reconstruction loss** and the **KL-divergence loss**. Use this training loop to train the model on the MNIST dataset, selecting choosing appropriate hyperparameters. Lastly, explore and analyze the latent encodings learned by the model through various visualization techniques.

![image](https://github.com/Di40/DeepLearning-Assignments/assets/57565142/899e45a5-bc14-4eb6-9734-684cf3543cf0)

![image](https://github.com/Di40/DeepLearning-Assignments/assets/57565142/d3883014-ae5b-4ad9-9d8b-a91664919c21)

